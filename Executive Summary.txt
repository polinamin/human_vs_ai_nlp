With ChatGPT playing a controversial role in how people acquire information, it is important to have methods for accurately predicting whether a response to a particular question is generated by a machine or a person. It is intuitive to note that questions differ in both structure (how words are placed in order to convey a message) and length (number of words in a question). For this reason, we want to design a model that can injest questions of any format and, looking at the provided responses, accurately determine whether the responses was generated by a human or Chat GPT. Additionally, how much a preprocessing step impacts the success of our model is unclear. Subsequently, all analysis was split into "with lemmatization" and "without", in order to determine how more sophisticated word analysis in a string would impact our ability to predict the author "type" of the string. 

What is evident is that lemmatization is a game-changer. Running a lemma analysis on responses, allowed for a simplification in the language strucure our strings; improving the model accuracy by 10 percentage points (from 0.90 to 0.996). Lemmatization likewiwse had positive effect on our ability to minimize false negatives and false positives, with sensitivity and specificity each increasing by 8 (from 0.92 to 1) and 11 (from 0.88 to 0.99) percentage points respectively.

Looking at our data, the model with the highest success rate is an ADA-boosted logistic regression model. This model results in the smallest difference between the train and test data, while also looking at achieving the highest success rate.

Moving forward, it'll be integral to continue training a "AI text recognition" model, in order to evolve and strengthen the list of differentiators between AI and Human writing.
